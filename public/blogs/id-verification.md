---
title: "Don't Blame Discord - Blame Your Legislature: ID Verification"
date: "2026-2-20"
categories: ["Security", "Privacy", "Law"]
tags: ["Security", "Privacy", "Law"]
cover: "id-verification.png"
---

## A Firestorm Aimed at the Wrong Target

The internet has been in an uproar. Discord's announcement that it would begin rolling out global age verification starting in [March 2026](https://techcrunch.com/2026/02/09/discord-to-roll-out-age-verification-next-month-for-full-access-to-its-platform/) - requiring face scans or government-issued IDs to access adult content, age-restricted channels, or to change default safety settings - ignited a rage that spread rapidly from Reddit threads to Mastodon instances to Lemmy communities, creating one of the biggest social media backlashes in recent memory. Users decried it as a betrayal. Some declared they were leaving Discord forever. Activists called for boycotts. Entire communities began debating whether to migrate to self-hosted Matrix servers, decentralized fediverses, or just abandon ship entirely.

Almost none of that anger was aimed at the right target.

This piece is about what's actually happening: who is really driving the age verification wave engulfing the internet, why platforms have no meaningful choice but to comply, why even the most principled non-compliance strategies eventually hit a wall, and why boycotting Discord or canceling your subscription to any platform does functionally nothing to address the root cause. It's also about the deeply troubling institutional reality that no company, no matter how large or small, no matter how well-staffed its legal team, can realistically keep pace with the volume, speed, and contradictions of the laws being passed - which is precisely why rushed, blunt, globally indiscriminate rollouts are not corporate callousness, but often the only rational engineering response to an impossible situation.

Understanding this requires understanding the true scale of what has happened to internet law in the past two years - and it is genuinely staggering.

## The Legislative Avalanche - What Has Actually Happened to the Law

To understand why platforms are scrambling, you need to understand what the regulatory landscape looked like five years ago versus today. For most of the internet's commercial life, the primary federal law governing children online was the Children's Online Privacy Protection Act, or COPPA, enacted in 1998. COPPA was designed for a different era - one before Facebook, before YouTube, before smartphones, before the concept of a platform that a billion people used simultaneously. It covered children under 13. That was essentially it. Beyond that, the internet was largely self-regulated, with platforms writing their own terms of service and setting their own community standards.

That world is gone. What replaced it is a legislative environment so complex, so fragmented, and so rapidly evolving that even dedicated compliance professionals openly describe it as a crisis.

In 2025 alone, [nine U.S. states saw age verification laws for adult content take effect](https://www.eff.org/deeplinks/2025/12/year-states-chose-surveillance-over-safety-2025-review), and roughly 30 bills were introduced across 18 states targeting minors' social media access. By the end of 2025, [roughly half of all U.S. states had enacted some form of age verification or age-gating requirement](https://www.eff.org/deeplinks/2025/12/age-verification-threats-across-globe-2025-review) for online platforms, with more expected to take effect in 2026. And this is only the United States. Simultaneously: the UK's Online Safety Act came into full enforcement in 2025 and applies to any platform accessible to UK users regardless of where that platform is headquartered. Australia enacted what observers described as the world's strictest social media restriction for under-16s. The EU published guidelines requiring major platforms to implement age verification by July 2026. France threatened bans. Canada accelerated its own children's online safety legislation. Germany, through the NetzDG framework, was pushing its own age-assurance requirements.

The result is not a unified global policy. It is dozens of overlapping, often directly contradictory legal requirements descending simultaneously on companies that have to serve a global user base with a single product. As legal analysts at the National Law Review [put it plainly](https://natlawreview.com/article/new-age-verification-reality-compliance-rapidly-expanding-state-regulatory), companies now face "a fast-growing patchwork of state laws with inconsistent requirements, evolving definitions, and uncertain enforcement outcomes driven by ongoing constitutional litigation." That's not a political opinion - that's a description of the technical compliance reality that every legal team at every online platform was grappling with throughout 2025 and into 2026.

The Supreme Court decision that turbocharged all of this came in June 2025. In *Free Speech Coalition v. Paxton*, the Court upheld Texas's age verification statute for adult content, ruling that states may require websites to verify users' ages before granting access to certain content. This gave state legislatures the green light to accelerate. And accelerate they did. The irony is profound: courts are simultaneously striking down many of these laws as unconstitutional violations of the First Amendment while other laws are sailing into enforcement. [Courts blocked many of the laws seeking to impose age-verification gates on social media](https://www.eff.org/deeplinks/2025/12/year-states-chose-surveillance-over-safety-2025-review) on First Amendment grounds even as others went into effect - which means the compliance landscape is not even stable. A platform could build an entire technical system to comply with a state law, spend hundreds of thousands of dollars implementing it, and then have that law enjoined by a court, leaving the system useless and the next law requiring something different. Lawyers at Hogan Lovells [described the situation](https://news.bloomberglaw.com/privacy-and-data-security/companies-face-influx-of-conflicting-age-verification-laws) succinctly: "There are so many questions to sort out for these companies just to know what's actually required."

The contradictions are not abstract. They are specific, deep, and practically maddening. Different states define "harmful content" differently - some definitions are so broad they could plausibly capture depictions of same-sex couples. Utah's App Store Accountability Act, Texas's App Store Accountability Act, and Louisiana's equivalent all passed in 2025 and impose obligations on app stores and developers, but [each defines "app store" and "app" differently](https://www.cslawreport.com/print_issue.thtml?uri=cyber-security-law-report/content/vol-11/no-32-aug-13-2025), meaning the same piece of software may be covered in one state but not another under the same category of law. Age thresholds differ - some laws set the cutoff at 13, others at 16, others at 18. Some require verification only for specific content types; others require it for all access, period. Virginia's law limits minors to one hour of social media per day. Utah restricts use between 10:30 PM and 6:30 AM. Some states require parental consent systems. Others require platforms to retain age-verification records. Others - directly contradicting the first group - prohibit retaining that same data. Some are enjoined while their appeals proceed. Others are in effect but being challenged. New York's SAFE for Kids Act rules are still being finalized through a rulemaking process whose deadline keeps shifting, with the actual implementation date set [at 180 days after the rules are finally published](https://www.insideprivacy.com/childrens-privacy/end-of-year-2025-state-and-federal-developments-in-minors-privacy/) - which means nobody knows when it will actually take effect. California enacted the Digital Age Assurance Act in October 2025, which applies to *operating system providers* rather than app stores, creating yet another compliance category [taking effect January 1, 2027](https://www.insideprivacy.com/childrens-privacy/end-of-year-2025-state-and-federal-developments-in-minors-privacy/).

This is not a problem that a compliance team can track in a spreadsheet. It is a moving, mutating, jurisdiction-by-jurisdiction legal labyrinth that is being actively modified in real time by legislatures, courts, and regulatory bodies simultaneously.

## The Impossibility of Granular Compliance - Why No Team Is Big Enough

Here is where users' frustration with platforms tends to underestimate the real structural problem: even the most sophisticated, best-resourced legal and engineering teams on the planet cannot reasonably track, interpret, and implement compliant systems for every jurisdiction on the timeline these laws demand.

Start with the tracking problem. By mid-2025, [at least 18 new age verification bills had been proposed in a single legislative session alone](https://news.bloomberglaw.com/privacy-and-data-security/companies-face-influx-of-conflicting-age-verification-laws). Not enacted - proposed. That means a compliance team monitoring legislative developments across 50 U.S. states, the EU's 27 member states, the UK, Canada, Australia, and other jurisdictions is dealing with hundreds of potential bills at any given moment, most of which will never pass, some of which will pass in amended form that differs significantly from the bill that was being tracked, and some of which will pass, be enjoined immediately, be revised, and re-pass before the original compliance system is even built. And each of those bills may contain technical requirements that are genuinely ambiguous - requiring legal interpretation that itself takes time and money and may prove wrong when tested in court.

Then there's the implementation problem. Actually building age verification into a product is not a simple feature toggle. It requires integration with third-party identity verification vendors (who themselves come with their own security and privacy risks, as we'll see). It requires changes to the user interface and user experience across every platform version - web, iOS, Android, and desktop clients. It requires back-end infrastructure to process and then delete (or not delete, depending on the jurisdiction) verification data. It requires compliance with data privacy laws like GDPR in Europe and the CCPA in California that simultaneously govern how that same data must be handled. It requires age-gating logic that differs by jurisdiction - one system for the UK, another for Australia, another for Mississippi, another for Florida - all running simultaneously for a global user base. It requires testing. It requires security audits. It requires legal review at each step. It requires coordination across engineering, legal, privacy, cybersecurity, and product design teams, all of whom have other priorities. And it requires doing all of this before the law's effective date - which may be months away, or may be weeks.

As Bluesky put it explicitly in its own [public statement](https://bsky.social/about/blog/08-22-2025-mississippi-hb1126) about why it couldn't comply with Mississippi's law: "Age verification systems require substantial infrastructure and developer time investments, complex privacy protections, and ongoing compliance monitoring - costs that can easily overwhelm smaller providers." Bluesky is not a tiny startup. It had significant venture backing and real engineering talent. It still found Mississippi's requirements beyond its immediate capacity - so it simply shut off access for all Mississippi users entirely, until courts resolve the underlying legal questions. That's what compliance failure looks like in the real world: not a company stubbornly refusing to do the right thing, but a company making a rational calculation that blocking an entire state is cheaper and faster than building the required system under the available timeline.

The cost figure circulating in policy circles is not made up: a nonprofit economic policy group called Engine [estimated that implementing age verification technology for even a small startup would cost at minimum $500,000](https://news.bloomberglaw.com/privacy-and-data-security/companies-face-influx-of-conflicting-age-verification-laws). That's before ongoing compliance monitoring, legal updates as laws change, security maintenance for the verification infrastructure, and incident response when (not if) something goes wrong. For a company like Dreamwidth - an open-source community platform for fanfiction, creative writing, and personal journals - that cost is essentially company-ending. Dreamwidth [chose to block all Mississippi users](https://www.eff.org/deeplinks/2025/09/age-verification-windfall-big-tech-and-death-sentence-smaller-platforms) entirely rather than attempt compliance, for the same reason. The Hamster Forum - a community literally dedicated to discussing pet hamsters - [announced the shutdown of its message boards entirely](https://www.eff.org/deeplinks/2025/09/age-verification-windfall-big-tech-and-death-sentence-smaller-platforms) in March 2025 because it could not afford UK Online Safety Act compliance. That is the scale of disruption these laws cause even to completely inoffensive, niche online communities with zero controversial content.

For larger companies, the math is somewhat different - they *can* absorb the costs - but the implementation challenge is no less severe. The issue for Discord, Reddit, YouTube, and their peers is not simply money. It is the engineering reality that a jurisdiction-by-jurisdiction compliance system, attempting to track which of the hundreds of laws is currently in effect, currently enjoined, currently amended, or currently awaiting rulemaking in each of the jurisdictions they serve, would itself be an extraordinarily complex piece of software requiring constant updates as the legal landscape shifts. [Compliance strategies must be "designed for change rather than permanence,"](https://natlawreview.com/article/new-age-verification-reality-compliance-rapidly-expanding-state-regulatory) as legal analysts have put it - which is an elegant way of saying that whatever system you build today may be wrong tomorrow not because you built it badly, but because the law moved. And it always moves.

This is the structural logic that makes a global, indiscriminate rollout like Discord's not just understandable, but arguably the only sane engineering choice available. Building one system - one age-inference model, one facial-age-estimation tool, one verification flow - and applying it globally is not lazy or cynical. It is the only way to create a product that can serve 200 million users without fracturing into dozens of jurisdiction-specific versions that would each require separate maintenance, separate legal review, separate security audits, and separate incident response as the legal landscape under each one continues to shift. When a new law passes in Texas, you don't want to be in a position where that triggers a six-month rebuild of the Texas-specific version of your product. You want your one global system to already satisfy whatever Texas now requires.

The brutal irony, as the [EFF has noted repeatedly](https://www.eff.org/deeplinks/2025/09/age-verification-windfall-big-tech-and-death-sentence-smaller-platforms), is that this dynamic disproportionately entrenches the platforms lawmakers claim to be targeting. Google and Meta have the resources to build sophisticated compliance infrastructure and absorb the costs. Discord can stretch to do it. Bluesky, Dreamwidth, and every other smaller, more innovative, often more privacy-respecting platform cannot - so they get crushed, blocked, or shuttered, while Big Tech grows stronger. The age verification laws, sold as a check on tech giants' power over children, are functionally a moat protecting those same tech giants from competition.

## Platforms Said All of This - Out Loud, Repeatedly, Before Complying

One of the most aggravating aspects of watching users direct their rage at Discord is the historical erasure it represents. Because the platforms did not stay silent about any of this. They said it. Loudly. Publicly. In official statements, blog posts, congressional testimony, and regulatory filings. The companies told you these laws were broken before they implemented them. Then the laws went into effect anyway, and now the platforms are blamed for what the laws forced them to do.

Bluesky's statement blocking Mississippi access is worth reading in full, not just quoting, because it is one of the clearest platform statements on this crisis that exists. The company [called the law out explicitly](https://bsky.social/about/blog/08-22-2025-mississippi-hb1126) for creating "significant barriers that limit free speech and disproportionately harm smaller platforms and emerging technologies." It noted that the law "would block everyone from accessing the site - teens and adults - unless they hand over sensitive information." It warned that compliance costs "can easily overwhelm smaller providers" and that "this dynamic entrenches existing big tech platforms while stifling the innovation and competition that benefits users." Then it blocked Mississippi anyway, because it had no better option.

Discord's own public statements and FAQ documentation are not the language of a company excited about its new ID collection regime. The company went to extraordinary lengths to explain its privacy protections, [repeatedly stressing that the "vast majority of people can continue using Discord exactly as they do today, without ever being asked to confirm their age"](https://support.discord.com/hc/en-us/articles/38332670254231-Age-Assurance-Update-FAQ) - which is true, because most users don't access age-restricted content and will be handled automatically by Discord's behavioral inference model. Discord's policy team carefully chose language emphasizing that IDs are deleted immediately, that facial scan data never leaves your device, that verification status is private. None of this is the posture of a company gleefully building a surveillance database. It's a company trying to manage something it didn't want to do.

Mastodon's founder Eugen Rochko was more blunt. He told the press plainly that [Mastodon "doesn't have the means" to comply](https://lemmy.ml/post/35396008) with Mississippi-style mandates and that the platform's entire founding philosophy was to enable social media to exist independently of American legal jurisdiction - precisely because he foresaw laws like these as a threat to the kind of open, self-governed internet communities Mastodon exists to serve. Even Roblox, which [rolled out mandatory facial verification](https://techcrunch.com/2026/02/09/discord-to-roll-out-age-verification-next-month-for-full-access-to-its-platform/) for platform chat access, publicly acknowledged it was doing so in direct response to regulatory pressure and state attorney general investigations.

Apple's CEO personally called the Governor of Texas to [argue against the App Store Accountability Act](https://www.cslawreport.com/print_issue.thtml?uri=cyber-security-law-report/content/vol-11/no-32-aug-13-2025) - and lost. The law passed anyway. Google lobbied hard against the same category of laws and [similarly failed](https://www.cslawreport.com/print_issue.thtml?uri=cyber-security-law-report/content/vol-11/no-32-aug-13-2025). The Software & Information Industry Association submitted congressional testimony warning that "age verification requires robust data collection, making it exceedingly difficult to minimize the sensitive data collected from youth users" and that "websites or platforms holding a rich array of sensitive data are more attractive targets for malicious actors, dramatically increasing the likelihood that a data breach will harm young people." That's not an EFF press release. That's [industry testimony](https://www.route-fifty.com/digital-government/2025/12/state-social-media-laws-run-familiar-challenges/409997/) warning lawmakers exactly what would happen if they passed these laws. The laws passed anyway.

Fight for the Future, a nonprofit digital rights organization, [framed it plainly](https://www.route-fifty.com/digital-government/2025/12/state-social-media-laws-run-familiar-challenges/409997/): "Lawmakers and tech companies are acting like online ID checks fencing off the internet are the only way forward, when in actuality, this is just another misstep in the current arc of censorship and surveillance on the internet." That quote is from a campaigner criticizing both lawmakers and platforms - but notably, not defending the status quo that led to age verification mandates, and recognizing that the platforms themselves were not the primary authors of this outcome.

## The Data Breach Problem - Why the Rushed Systems Are Already Failing

If you want to understand why the speed and indiscriminate nature of these rollouts is so dangerous, you need to understand what happened to Discord in October 2025. Because Discord did not announce a global ID verification system from a position of security and confidence. It announced one in the immediate aftermath of a data breach - one caused directly by a prior age verification system - that exposed the government IDs of approximately [70,000 users](https://www.nbcnews.com/tech/tech-news/70000-government-id-photos-exposed-discord-user-hack-rcna236714).

Here is what happened. Discord had a system for age verification appeals - cases where a user was incorrectly flagged as underage and wanted to prove their actual age. Users submitted government ID documents (passports, driver's licenses) to a third-party customer support provider. In September and October 2025, a hacking group identifying itself as Scattered Lapsus$ Hunters [gained access to this provider's systems](https://discord.com/press-releases/update-on-security-incident-involving-third-party-customer-service) and exfiltrated roughly 1.6 terabytes of data, including what they claimed were over 2 million government ID images. Discord confirmed that approximately 70,000 users' government ID photos were exposed. The company named the vendor 5CA, a Netherlands-based customer experience firm, though 5CA [denied wrongdoing and disputed the characterization](https://www.bitdefender.com/en-us/blog/hotforsecurity/discord-data-breach-5ca-leak-70000-ids) of how it handled Discord's data. The EFF [awarded Discord its 2025 "We Still Told You So" Breachies Award](https://www.eff.org/deeplinks/2026/02/discord-voluntarily-pushes-mandatory-age-verification-despite-recent-data-breach) - a grim honor for organizations that failed in the exact way critics had predicted they would.

The breach illustrated something cybersecurity professionals have been warning about for years: the moment you create a requirement for users to submit government-grade identity documents to access an online service, you create a high-value, highly sensitive database that becomes an attractive target. [Unlike passwords, uploaded driver's licenses and passports are not rotatable](https://cypherleap.com/discord-third-party-data-breach-2025/). You cannot issue yourself a new passport because the old one was hacked. The breach exposed permanently identifying information that affected users will carry for the rest of their lives. And as the Center for Democracy and Technology's [Aliya Bhatia noted](https://www.bankinfosecurity.com/discord-vendor-hack-exposes-id-data-in-ransom-bid-a-29661): "This case lays bare the privacy risks of even so-called 'privacy-protective' age-assurance approaches."

The critical insight from cybersecurity experts in the aftermath was that Discord's breach was not a failure unique to Discord - it was a [predictable consequence of creating these databases at all](https://www.bankinfosecurity.com/discord-vendor-hack-exposes-id-data-in-ransom-bid-a-29661). The security community described it as likely part of "a proliferating wave of hacks against databases created by new laws meant to shield minors from inappropriate content." Just months earlier, the Tea app - a women's dating safety app that required ID verification - was hacked, [exposing 72,000 images](https://www.nbcnews.com/tech/tech-news/70000-government-id-photos-exposed-discord-user-hack-rcna236714). The pattern is clear: mandate ID collection, create ID databases, watch those databases get compromised.

Now, in direct response to this breach, Discord launched a new global verification rollout using a *different* third-party vendor - k-ID - with promises that IDs are deleted immediately and facial scan data never leaves devices. [Proton's security analysts pointed out the obvious](https://proton.me/blog/discord-global-age-verification): while the vendor has changed, the underlying risk structure has not. Platforms are closed-source. Audits are limited. History shows repeatedly that data - especially the ultra-valuable, non-rotatable identity data that government IDs represent - will eventually leak, whether through hacks, misconfigurations, insider threats, or retention mistakes. The only way to truly prevent identity data from being breached is to not collect it. But the laws require collecting it. So the platforms collect it, the security community warns they shouldn't, the breach happens, the laws still don't change, and the cycle continues.

This is perhaps the most damning indictment of the rushed compliance timeline. Had governments allowed platforms meaningful time to develop privacy-preserving age assurance alternatives - approaches that can verify a user's age range without storing the underlying identity documents - the breach landscape might look very different. Instead, platforms implementing compliance systems rapidly, under tight legal deadlines, under the threat of massive fines, with ambiguous technical requirements, are inevitably going to implement them imperfectly. The speed demanded by law does not allow for the security rigor that the data being collected demands.

## The Fediverse and Self-Hosted Platforms - A Closing Door, Not an Open Window

Many users have responded to Discord's announcement by pointing toward decentralized alternatives - Mastodon, Matrix, Lemmy, self-hosted instances of various platforms - as a principled escape hatch. And it's true that for now, many self-hosted and federated platforms exist in a kind of regulatory shadow where enforcement has been limited. But the evidence is strongly accumulating that this window is closing, and anyone treating it as a permanent solution is underestimating the momentum of the regulatory apparatus being built.

Mastodon's situation illustrates the tension clearly. The Mastodon 4.4 release in July 2025 added [administrator tools for specifying minimum ages and handling terms of service](https://disassociated.com/mastodon-struggle-comply-social-media-age-verification-laws/), explicitly in response to emerging regulations. But Mastodon's fundamental architecture makes compliance deeply problematic - the platform's software [doesn't retain age-verification data](https://disassociated.com/mastodon-struggle-comply-social-media-age-verification-laws/) even when administrators implement age checks, because storing it was explicitly designed out of the system as a privacy feature. Mastodon has no central authority that can make compliance decisions on behalf of the entire federation - as Rochko himself [told reporters](https://lemmy.ml/post/35396008), "there is nobody that can decide for the fediverse to block Mississippi." Every individual instance operator must make their own compliance decision, under their own jurisdiction's laws, without the legal team, engineering resources, or financial reserves of a centralized platform.

What this means in practice is that a self-hosted Mastodon instance running in the UK is subject to the Online Safety Act in the same way Discord is - including Ofcom's enforcement powers. The OSA [applies to any service that allows users to post content online or interact with each other](https://www.gov.uk/government/publications/online-safety-act-explainer/online-safety-act-explainer), regardless of where that service is physically located, if it is accessible to UK users. A small self-hosted forum in someone's basement in Manchester is theoretically in scope. An instance run by a hobbyist in Germany that UK users can reach is theoretically in scope. Whether enforcement reaches such small operators in practice is a separate question - but the legal exposure is real and documented.

Ofcom has demonstrated it is serious about reaching beyond large platforms. It established a dedicated ["Small but Risky Services Taskforce"](https://www.ofcom.org.uk/online-safety/illegal-and-harmful-content/online-safety-industry-bulletins/online-safety-industry-bulletin-december-2025) specifically to "tackle the distinct challenges posed by online platforms with small userbases but high potential for harm." By October 2025, the regulator had [launched five enforcement programmes and opened 21 investigations](https://www.dlapiper.com/en/insights/publications/2025/10/online-safety-act), including against platforms that had attempted to geo-block UK users to avoid OSA scope - and had begun [taking enforcement action against some that failed to maintain effective geo-blocks](https://www.dlapiper.com/en/insights/publications/2025/10/online-safety-act). In the US, the jurisdictional reach of state laws to foreign-hosted or self-hosted services remains contested in court - a US-based image board site [filed a formal jurisdictional challenge](https://www.burges-salmon.com/articles/102mi1g/ofcom-and-the-online-safety-act-in-2026/) against Ofcom's attempt to enforce the OSA against it in 2025.

For individual self-hosted operators, the more near-term risk may not be direct regulatory enforcement but infrastructure pressure. Regulators and governments have historically shown willingness to approach compliance failures at the infrastructure level: pressuring hosting providers, domain registrars, app stores, and CDN providers to enforce compliance on their hosted services. [Community discussion on Lemmy](https://lemmy.ml/post/35396008) has already noted the likely escalation path: if regulators cannot practically enforce against thousands of individual federated instances, the logical next step is to go after the software developers and repository hosts as "facilitators of non-compliance" - a model derived from intellectual property enforcement campaigns that targeted software tools rather than end users. Whether that happens depends on political will, not legal impossibility.

The honest assessment is this: self-hosted and federated platforms represent meaningful tools for communities that are technically sophisticated and operate in jurisdictions with weaker enforcement. They are not a solution to the underlying legal and political problem. As a community in [Lemmy put it](https://lemmy.ml/post/43290621): the regulatory regimes will succeed at slipping these oppressive laws through in part because people who should be fighting the laws are instead retreating to technical workarounds and leaving the political fight to others.

## The Consolidation Effect - How These Laws Help the Platforms Users Hate

There is one more layer to this story that deserves sustained attention, because it is deeply counterintuitive but well-documented: the age verification laws that users often see as Big Tech capitulating to government pressure are, in practice, enormously beneficial to the largest platforms at the expense of everyone else.

Consider the mechanics. A law that requires every social platform to build age verification infrastructure, parental consent workflows, data deletion systems, and ongoing compliance monitoring creates a massive fixed cost. For Meta, Google, or even Discord, that cost is substantial but absorbable - these companies have legal departments, engineering teams, and compliance infrastructure already in place. They can build the required systems, pay for the legal analysis, contract with reputable third-party verification vendors, and absorb the capital expenditure. For Bluesky, Dreamwidth, Mastodon, or any of the thousands of smaller platforms, forums, and communities that constitute the actual diversity of the internet, that same fixed cost is potentially existential.

This is not speculation - it's exactly what happened in the UK when the Online Safety Act began to bite in 2025. Large platforms like Reddit, YouTube, and Spotify [implemented compliance measures](https://www.eff.org/deeplinks/2025/09/age-verification-windfall-big-tech-and-death-sentence-smaller-platforms), however clumsily. Forums on parenting, green living, gaming on Linux, and pet care were forced to shut down their community boards because the compliance cost was beyond them. The Hamster Forum closed. Niche communities that served real people in ways that large platforms never could were simply eliminated by the cost of compliance with laws notionally designed to protect them. The EFF described this plainly: age verification mandates [concentrate and consolidate power in the hands of the largest companies](https://www.eff.org/deeplinks/2025/09/age-verification-windfall-big-tech-and-death-sentence-smaller-platforms) - the only entities with resources to build costly compliance systems and absorb potentially massive fines - while pushing smaller, often better communities offline.

The result is that every time a state or national government passes an age verification law, it inadvertently runs a consolidation program for the very tech giants lawmakers claim to be holding accountable. Users who cancel Discord in protest and migrate to smaller federated platforms are pushing those platforms toward the same compliance cliff. Users who stay on Discord - even while hating its verification system - are at least supporting a platform that has the resources to weather the storm, however imperfectly. Neither choice addresses the actual problem. Neither choice changes the underlying law. And the underlying law is what's doing all of this damage.

## The Surveillance Infrastructure Question - The Long Game

Digital rights advocates' deepest concern about all of this is not really about any specific platform or any specific law. It is about the infrastructure being built and what it will be used for once it exists.

The EU's "mini-AV" application, rushed out in 2025 ahead of the full EU Digital Identity Wallet rollout, [already has the infrastructure in place to expand identity checks beyond age verification](https://www.eff.org/deeplinks/2025/04/digital-identities-and-future-age-verification-europe) to other purposes. The Commission has acknowledged this. The EU Digital Identity Wallet, rolling out more broadly in 2026, is designed for age verification but is architecturally capable of much more. Once the infrastructure exists to require identity verification before accessing online services, the scope of what requires verification is a political question - not a technical one. And political questions can be answered differently by different governments, at different times, under different pressures.

The EFF has [warned consistently](https://www.eff.org/deeplinks/2025/12/10-not-so-hidden-dangers-age-verification) that age verification systems are, at their core, surveillance systems. "By requiring identity verification to access basic online services, we risk creating an internet where anonymity is a thing of the past." The chilling effect documented by advocates is not hypothetical: when LGBTQ+ youth, survivors of abuse, political dissidents, or anyone else who relies on online anonymity for personal safety knows that their real-world identity is linked to their online activity through a government ID verification process, they speak differently - or they don't speak at all. The EFF [notes explicitly](https://www.eff.org/deeplinks/2026/02/discord-voluntarily-pushes-mandatory-age-verification-despite-recent-data-breach) that for the 43 percent of transgender Americans who lack identity documents accurately reflecting their name or gender, age verification creates an impossible choice: provide documents that out them, or lose access to the online communities where they may be safest.

Research has confirmed what advocates predicted and what data now documents: [age verification laws don't work](https://www.eff.org/deeplinks/2025/12/year-states-chose-surveillance-over-safety-2025-review). Studies from the New York Center for Social Media and Politics and the Phoenix Center found that when platforms blocked access under these laws, searches for platforms that had blocked access dropped - but searches for offshore sites surged. Florida saw a [1,150% increase in VPN demand](https://en.wikipedia.org/wiki/Social_media_age_verification_laws_in_the_United_States) after its age verification law took effect. Children determined to access content routed around the restrictions. Adults committed to privacy used VPNs. The platforms that complied lost users. The platforms that didn't comply gained them. And the unregulated, less safe corners of the internet grew.

The laws accomplish one thing reliably: they build the infrastructure of identity verification into the baseline of internet access. Once built, that infrastructure doesn't disappear. It expands.

## Where to Actually Direct Your Energy

None of this is a claim that corporations deserve uncritical trust with your data. Discord's verification system carries real privacy risks, and reasonable people can make reasonable choices to minimize their personal exposure - using the facial estimation rather than ID upload, using pseudonymous accounts for different communities, or migrating activity to end-to-end encrypted alternatives for sensitive conversations. Skepticism about corporate data handling is healthy and warranted.

But boycotting Discord accomplishes nothing beyond making Discord's compliance costs slightly cheaper. The company cannot reverse these changes unilaterally. The UK's Online Safety Act [carries fines of up to Â£18 million or 10% of global annual revenue](https://www.burges-salmon.com/articles/102mi1g/ofcom-and-the-online-safety-act-in-2026/) for non-compliance, and Ofcom has already demonstrated it will use those powers. The US state laws carry fines of up to [$10,000 per user violation](https://bsky.social/about/blog/08-22-2025-mississippi-hb1126) in some jurisdictions. These are not threats that any company with 200 million users can ignore or absorb through non-compliance. The decision that led to Discord's verification rollout was made by legislators - in Westminster, in Canberra, in Jackson, in Tallahassee, in Sacramento - not in Discord's San Francisco headquarters.

The EFF has built a comprehensive resource hub at [EFF.org/Age](https://www.eff.org/deeplinks/2025/12/age-verification-coming-internet-we-built-you-resource-hub-fight-back) for anyone who wants to understand these laws, challenge them, or support the legal efforts to overturn them. Multiple laws are actively being challenged in court, and some have been successfully blocked. The fight is not over. But it has to be fought in the right place - in the courts, in the legislatures, in the regulatory comment periods where digital rights advocates are still submitting evidence and making arguments that sometimes win.

Discord didn't do this to you. Your government did. And until that distinction becomes the focus of the outrage, nothing meaningful will change.